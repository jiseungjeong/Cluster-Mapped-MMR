{
  "all_results_summary": {
    "5": {
      "commonsenseqa": {
        "random": {
          "count": 100,
          "accuracy": 0.77,
          "tokens": {},
          "response_time": {
            "mean": 1.5329114770889283,
            "std": 0.3878529248393553
          },
          "selection_time": {
            "mean": 1.9643306732177735e-05,
            "std": 4.642305403419975e-06
          },
          "total_latency": {
            "mean": 1.5329311203956604,
            "std": 0.3878537908241372
          }
        }
      }
    }
  },
  "args": {
    "dataset": "commonsenseqa",
    "data_dir": "./data",
    "num_test_samples": 300,
    "num_repeats": 10,
    "method": "random",
    "num_examples": [
      5
    ],
    "lambda_param": 0.7,
    "n_clusters": 5,
    "min_cluster_size": 5,
    "min_samples": null,
    "embedding_model": "all-mpnet-base-v2",
    "force_recompute": false,
    "gpt_model": null,
    "gpt_temperature": null,
    "gpt_max_tokens": null,
    "experiment_name": "commonsenseqa_random_20250522_185936",
    "results_dir": "./results"
  },
  "repeat_index": 3
}